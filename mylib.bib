@article{Borji2014,
abstract = {Detecting and segmenting salient objects in natural scenes, often referred to as salient object detection, has attracted a lot of interest in computer vision. While many models have been proposed and several applications have emerged, yet a deep understanding of achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics in salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance and suggest future research directions.},
archivePrefix = {arXiv},
arxivId = {1411.5878},
author = {Borji, Ali and Cheng, Ming-Ming and Hou, Qibin and Jiang, Huaizu and Li, Jia},
eprint = {1411.5878},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Borji et al. - 2014 - Salient Object Detection A Survey.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {nov},
title = {{Salient Object Detection: A Survey}},
year = {2014}
}
@article{Bouwmans2014a,
abstract = {Background modeling for foreground detection is often used in different applications to model the background and then detect the moving objects in the scene like in video surveillance. The last decade witnessed very significant publications in this field. Furthermore, several surveys can be found in the literature but none of them addresses an overall review in this field. So, the purpose of this paper is to provide a complete survey of the traditional and recent approaches. First, we categorize the different approaches found in the literature. We have classified them in terms of the mathematical models used and we have discussed them in terms of the critical situations that they claim to handle. Furthermore, we present the available resources, datasets and libraries. Then, we conclude with several promising directions for future research.},
author = {Bouwmans, Thierry},
doi = {10.1016/J.COSREV.2014.04.001},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouwmans - 2014 - Traditional and recent approaches in background modeling for foreground detection An overview(2).pdf:pdf},
issn = {1574-0137},
journal = {Computer Science Review},
mendeley-groups = {Prosjektoppgave},
month = {may},
pages = {31--66},
publisher = {Elsevier},
title = {{Traditional and recent approaches in background modeling for foreground detection: An overview}},
volume = {11-12},
year = {2014}
}
@article{Bouwmans2014,
abstract = {Foreground detection is the first step in video surveillance system to detect moving objects. Recent research on subspace estimation by sparse representation and rank minimization represents a nice framework to separate moving objects from the background. Robust Principal Component Analysis (RPCA) solved via Principal Component Pursuit decomposes a data matrix A in two components such that A=L+S, where L is a low-rank matrix and S is a sparse noise matrix. The background sequence is then modeled by a low-rank subspace that can gradually change over time, while the moving foreground objects constitute the correlated sparse outliers. To date, many efforts have been made to develop Principal Component Pursuit (PCP) methods with reduced computational cost that perform visually well in foreground detection. However, no current algorithm seems to emerge and to be able to simultaneously address all the key challenges that accompany real-world videos. This is due, in part, to the absence of a rigorous quantitative evaluation with synthetic and realistic large-scale dataset with accurate ground truth providing a balanced coverage of the range of challenges present in the real world. In this context, this work aims to initiate a rigorous and comprehensive review of RPCA-PCP based methods for testing and ranking existing algorithms for foreground detection. For this, we first review the recent developments in the field of RPCA solved via Principal Component Pursuit. Furthermore, we investigate how these methods are solved and if incremental algorithms and real-time implementations can be achieved for foreground detection. Finally, experimental results on the Background Models Challenge (BMC) dataset which contains different synthetic and real datasets show the comparative performance of these recent methods.},
author = {Bouwmans, Thierry and Zahzah, El Hadi},
doi = {10.1016/J.CVIU.2013.11.009},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouwmans, Zahzah - 2014 - Robust PCA via Principal Component Pursuit A review for a comparative evaluation in video surveillance(2).pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
mendeley-groups = {Prosjektoppgave},
month = {may},
pages = {22--34},
publisher = {Academic Press},
title = {{Robust PCA via Principal Component Pursuit: A review for a comparative evaluation in video surveillance}},
volume = {122},
year = {2014}
}
@article{R-FCN,
abstract = {We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6{\%} mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn},
archivePrefix = {arXiv},
arxivId = {1605.06409},
author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
eprint = {1605.06409},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai et al. - 2016 - R-FCN Object Detection via Region-based Fully Convolutional Networks.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {may},
title = {{R-FCN: Object Detection via Region-based Fully Convolutional Networks}},
year = {2016}
}
@article{Dalal,
abstract = {We study the question of feature sets for robust visual ob-ject recognition, adopting linear SVM based human detec-tion as a test case. After reviewing existing edge and gra-dient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors sig-nificantly outperform existing feature sets for human detec-tion. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping de-scriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
author = {Dalal, Navneet and Triggs, Bill},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalal, Triggs - Unknown - Histograms of Oriented Gradients for Human Detection.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Histograms of Oriented Gradients for Human Detection}},
}
@article{Everinghama,
abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
author = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-009-0275-4},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Everingham et al. - Unknown - The PASCAL Visual Object Classes (VOC) Challenge(2).pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition},
mendeley-groups = {Prosjektoppgave},
number = {2},
pages = {303--338},
pmid = {20713396},
title = {{The pascal visual object classes (VOC) challenge}},
volume = {88},
year = {2010}
}
@article{FastR-CNN,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
eprint = {1504.08083},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick - 2015 - Fast R-CNN.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Fast R-CNN}},
year = {2015}
}
@article{R-CNN,
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30{\%} relative to the previous best result on VOC 2012---achieving a mAP of 53.3{\%}. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\~{}}rbg/rcnn.},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
eprint = {1311.2524},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2013 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {nov},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2013}
}
@article{Gunn1998,
author = {Gunn, Steve R},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gunn - 1998 - UNIVERSITY OF SOUTHAMPTON Support Vector Machines for Classification and Regression.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{UNIVERSITY OF SOUTHAMPTON Support Vector Machines for Classification and Regression}},
year = {1998}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
eprint = {1703.06870},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2017 - Mask R-CNN.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {mar},
title = {{Mask R-CNN}},
year = {2017}
}
@article{He,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 clas-sification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first 1 to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{Huang,
abstract = {The goal of this paper is to serve as a guide for se-lecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern con-volutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base fea-ture extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [31], R-FCN [6] and SSD [26] systems, which we view as " meta-architectures " and trace out the speed/accuracy trade-off curve created by using alterna-tive feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and mem-ory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance mea-sured on the COCO detection task.},
author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin and Research, Google},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - Speedaccuracy trade-offs for modern convolutional object detectors.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Speed/accuracy trade-offs for modern convolutional object detectors}},
year = {2017}
}
@article{Janai2017,
abstract = {Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
archivePrefix = {arXiv},
arxivId = {1704.05519},
author = {Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
eprint = {1704.05519},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janai et al. - 2017 - Computer Vision for Autonomous Vehicles Problems, Datasets and State-of-the-Art.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {apr},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art}},
year = {2017}
}
@article{Jiang,
abstract = {Enabling intelligent devices to accurately detect and identify the environment with various disturbances is very important. In this final report, we present an object detec-tion approach for low quality videos. Still images with mo-tion blur, Gaussian blur and pixelation are trained in addi-tion to clean images using both faster R-CNN and YOLO algorithms. The models are integrated with low resolu-tion webcam videos and evaluated with an object counting agent. The final models demonstrate improvements in both low quality images and videos. The cause of errors are dis-cussed.},
author = {Jiang, Hao and Wang, Shiquan},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Wang - Unknown - Object Detection and Counting with Low Quality Videos.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Object Detection and Counting with Low Quality Videos}},
}
@article{Jiang2016,
abstract = {A new Bayesian state and parameter learning algorithm for multiple target tracking (MTT) models with image observations is proposed. Specifically, a Markov chain Monte Carlo algorithm is designed to sample from the posterior distribution of the unknown num-ber of targets, their birth and death times, states and model parameters, which constitutes the complete solution to the tracking problem. The conventional approach is to pre-process the images to extract point observations and then perform tracking. We model the image generation process directly to avoid potential loss of information when extracting point observations. Numerical examples show that our algorithm has improved tracking per-formance over commonly used techniques, for both synthetic examples and real florescent microscopy data, especially in the case of dim targets with overlapping illuminated regions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.05522v1},
author = {Jiang, Lan and Singh, Sumeetpal S},
eprint = {arXiv:1603.05522v1},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Singh - 2016 - Tracking multiple moving objects in images using Markov Chain Monte Carlo.pdf:pdf},
keywords = {()},
mendeley-groups = {Prosjektoppgave},
title = {{Tracking multiple moving objects in images using Markov Chain Monte Carlo}},
year = {2016}
}
@article{Karpathy2016,
abstract = {We present a model that generates natural language de-scriptions of images and their regions. Our approach lever-ages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between lan-guage and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architec-ture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in re-trieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions sig-nificantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
author = {Karpathy, Andrej and Fei-Fei, Li},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - Unknown - Deep Visual-Semantic Alignments for Generating Image Descriptions.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
year = {2016}
}
@article{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - Unknown - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}
@misc{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - Unknown - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Kumar,
abstract = {Moving object segmentation in maritime domain is a challenging task due to the various real time practical problems, such as waves on the water surface, boat wakes and weather issues (such as bright sun, fog and heavy rain). These problems contribute to generate a highly dynamic back-ground, gradual and sudden illumination changes, camera jitter, shadows and reflections that can provoke false detections. To deal with above issues, in this paper a fast and robust moving object segmentation method on a water surface for maritime surveillance using dynamic background modeling and shadow suppression (OSDBMSS) in the complex wavelet domain is proposed. For dynamic background modeling, we have used frame difference, background registration, back-ground difference and background difference mask in the complex wavelet domain. For shadow detection and removal, we exploit the high frequency subband in the complex wavelet domain. A comparative analysis of the proposed method is presented both qualitatively and quantitatively with other standard methods available in the literature for publicly available datasets of videos in differ-ent maritime scenarios, with varying light and weather conditions. Experimental results indicate that the proposed method is performing better in comparison to other standard methods for all the test cases.},
author = {Kumar, Alok and Kushwaha, Singh and Srivastava, Rajeev},
doi = {10.1093/comjnl/bxv091},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar, Kushwaha, Srivastava - Unknown - Maritime Object Segmentation Using Dynamic Background Modeling and Shadow Suppression.pdf:pdf},
keywords = {change and shadow detection,complex wavelet domain,dynamic background modeling,maritime video surveillance,moving object segmentation},
mendeley-groups = {Prosjektoppgave},
title = {{Maritime Object Segmentation Using Dynamic Background Modeling and Shadow Suppression}},
url = {https://watermark.silverchair.com/bxv091.pdf?token=AQECAHi208BE49Ooan9kkhW{\_}Ercy7Dm3ZL{\_}9Cf3qfKAc485ysgAAAdgwggHUBgkqhkiG9w0BBwagggHFMIIBwQIBADCCAboGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM28lJSvDFTzeoqjhUAgEQgIIBiz1nUJICPoMrXEZHkZSQW8h-mCV9L6rPyFD0g0qVljYhGMhq}
}
@article{LeCun1988,
author = {le Cun, Yann},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/le Cun - 1988 - A Theoretical Framework for Back-Propagation.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{A Theoretical Framework for Back-Propagation}},
year = {1988}
}
@article{Lin2016,
abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1612.03144},
author = {Lin, Tsung-Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
eprint = {1612.03144},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2016 - Feature Pyramid Networks for Object Detection.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {dec},
title = {{Feature Pyramid Networks for Object Detection}},
year = {2016}
}
@article{Lin2017,
abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.},
archivePrefix = {arXiv},
arxivId = {1708.02002},
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
eprint = {1708.02002},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2017 - Focal Loss for Dense Object Detection.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {aug},
title = {{Focal Loss for Dense Object Detection}},
year = {2017}
}
@article{Lin2017a,
abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors.},
annote = {Why are one-stage detector less accurate than two stage? 

two stage meaning a classifier working on a set of region proposals.},
archivePrefix = {arXiv},
arxivId = {1708.02002},
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
eprint = {1708.02002},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2017 - Focal Loss for Dense Object Detection.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {aug},
title = {{Focal Loss for Dense Object Detection}},
year = {2017}
}
@article{SSD,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - SSD Single Shot MultiBox Detector.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {dec},
title = {{SSD: Single Shot MultiBox Detector}},
year = {2016}
}
@article{YOLOv1,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to per-form detection. Instead, we frame object detection as a re-gression problem to spatially separated bounding boxes and associated class probabilities. A single neural network pre-dicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detec-tors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other de-tection methods, including DPM and R-CNN, when gener-alizing from natural images to other domains like artwork.},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon et al. - Unknown - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
year = {2016}
}
@article{YOLOv2,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Us-ing a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on ob-ject detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts de-tections for more than 9000 different object categories. And it still runs in real-time.},
author = {Redmon, Joseph and Farhadi, Ali},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - Unknown - YOLO9000 Better, Faster, Stronger.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{YOLO9000: Better, Faster, Stronger}},
year = {2016}
}
@article{FasterR-CNN,
abstract = {—State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with " attention " mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
keywords = {Convolutional Neural Network,Index Terms—Object Detection,Region Proposal},
mendeley-groups = {Prosjektoppgave},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2016}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sermanet et al. - 2013 - OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {dec},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
year = {2013}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for ar-tificial intelligence due to its enormous search space and the difficulty of evaluating board positions and moves. We introduce a new approach to computer Go that uses value networks to evaluate board positions and policy networks to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte-Carlo tree search programs that sim-ulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte-Carlo simulation with value and policy networks. Using this search al-gorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the European Go champion by 5 games to 0. This is the first time that a com-puter program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. All games of perfect information have an optimal value function, v * (s), which determines the outcome of the game, from every board position or state s, under perfect play by all players. These games may be solved by recursively computing the optimal value function in a search tree containing approximately b d possible sequences of moves, where b is the game's breadth (number 1},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - Unknown - Mastering the Game of Go with Deep Neural Networks and Tree Search.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Mastering the Game of Go with Deep Neural Networks and Tree Search}},
year = {2016}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facili-tate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf:pdf},
keywords = {()},
mendeley-groups = {Prosjektoppgave},
title = {{VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION}},
year = {2015}
}
@article{SinghKushwaha2016,
author = {{Singh Kushwaha}, Alok Kumar and Srivastava, Rajeev},
doi = {10.1093/comjnl/bxv091},
issn = {0010-4620},
journal = {The Computer Journal},
month = {sep},
number = {9},
pages = {1303--1329},
publisher = {Oxford University Press},
title = {{Maritime Object Segmentation Using Dynamic Background Modeling and Shadow Suppression}},
volume = {59},
year = {2016}
}
@article{SinghKushwaha2016a,
author = {{Singh Kushwaha}, Alok Kumar and Srivastava, Rajeev},
doi = {10.1093/comjnl/bxv091},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh Kushwaha, Srivastava - 2016 - Maritime Object Segmentation Using Dynamic Background Modeling and Shadow Suppression.pdf:pdf},
issn = {0010-4620},
journal = {The Computer Journal},
month = {sep},
number = {9},
pages = {1303--1329},
publisher = {Oxford University Press},
title = {{Maritime Object Segmentation Using Dynamic Background Modeling and Shadow Suppression}},
volume = {59},
year = {2016}
}
@article{Snidaro,
abstract = {The problem of finding an automatic thresholding technique is well known in applications involving image differ-encing like visual-based surveillance systems, autonomous vehicle driving, etc. Among the algorithms proposed in the past years, the thresholding technique based on the stable Euler number method is considered one of the most promising in terms of visual results. Unfortunately its high computational complexity made it an impossible choice for real-time applications. The implementation here proposed, called fast Euler numbers, overcomes the problem since it calculates all the Euler numbers in just one single raster scan of the image. That is, it runs in OðhwÞ, where h and w are the image{\~{O}}s height and width, respectively. A technique for determining the optimal threshold, called zero crossing, is also proposed.},
author = {Snidaro, L and Foresti, G L},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snidaro, Foresti - Unknown - Real-time thresholding with Euler numbers.pdf:pdf},
keywords = {Automatic thresholding,Euler number,Optimal threshold,Real-time,Video surveillance},
mendeley-groups = {Prosjektoppgave},
title = {{Real-time thresholding with Euler numbers}},
}
@article{Sobral2014,
abstract = {Background subtraction (BS) is a crucial step in many computer vision systems, as it is first applied to detect moving objects within a video stream. Many algorithms have been designed to segment the foreground objects from the background of a sequence. In this article, we propose to use the BMC (Background Models Challenge) dataset, and to compare the 29 methods implemented in the BGSLibrary. From this large set of various BG methods, we have conducted a relevant experimental analysis to evaluate both their robustness and their practical performance in terms of processor/memory requirements.},
author = {Sobral, Andrews and Vacavant, Antoine},
doi = {10.1016/J.CVIU.2013.12.005},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sobral, Vacavant - 2014 - A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos(2).pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
mendeley-groups = {Prosjektoppgave},
month = {may},
pages = {4--21},
publisher = {Academic Press},
title = {{A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos}},
volume = {122},
year = {2014}
}
@article{Szegedy,
abstract = {We propose a deep convolutional neural network ar-chitecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the compu-tational budget constant. To optimize quality, the architec-tural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going Deeper with Convolutions.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Going Deeper with Convolutions}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2015/papers/Szegedy{\_}Going{\_}Deeper{\_}With{\_}2015{\_}CVPR{\_}paper.pdf}
}
@article{Szegedy2014,
abstract = {Current high-quality object detection approaches use the scheme of salience-based object proposal methods followed by post-classification using deep convolutional features. This spurred recent research in improving object proposal methods. However, domain agnostic proposal generation has the principal drawback that the proposals come unranked or with very weak ranking, making it hard to trade-off quality for running time. This raises the more fundamental question of whether high-quality proposal generation requires careful engineering or can be derived just from data alone. We demonstrate that learning-based proposal methods can effectively match the performance of hand-engineered methods while allowing for very efficient runtime-quality trade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox) approach, we substantially advance the state-of-the-art on the ILSVRC 2014 detection challenge data set, with {\$}0.5{\$} mAP for a single model and {\$}0.52{\$} mAP for an ensemble of two models. MSC-Multibox significantly improves the proposal quality over its predecessor MultiBox{\~{}}method: AP increases from {\$}0.42{\$} to {\$}0.53{\$} for the ILSVRC detection challenge. Finally, we demonstrate improved bounding-box recall compared to Multiscale Combinatorial Grouping with less proposals on the Microsoft-COCO data set.},
archivePrefix = {arXiv},
arxivId = {1412.1441},
author = {Szegedy, Christian and Reed, Scott and Erhan, Dumitru and Anguelov, Dragomir and Ioffe, Sergey},
eprint = {1412.1441},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Scalable, High-Quality Object Detection.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {dec},
title = {{Scalable, High-Quality Object Detection}},
year = {2014}
}
@article{Tangstad2017,
author = {Tangstad, Espen Johansen},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tangstad - 2017 - Visual Detection of Maritime Vessels.pdf:pdf},
keywords = {Kybernetikk og robotikk (2 {\aa}rig),Roboter og fat{\o}ystyring},
mendeley-groups = {Prosjektoppgave},
publisher = {NTNU},
title = {{Visual Detection of Maritime Vessels}},
year = {2017}
}
@article{SelSearch,
abstract = {This paper addresses the problem of generating possible object lo-cations for use in object recognition. We introduce Selective Search which combines the strength of both an exhaustive search and seg-mentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to gen-erate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our Selective Search results in a small set of data-driven, class-independent, high quality locations, yielding 99{\%} recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The Selective Search software is made publicly available 1 .},
author = {Uijlings, J R R and {Van De Sande}, K E A and Gevers, T and Smeulders, A W M},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Uijlings et al. - Unknown - Selective Search for Object Recognition.pdf:pdf},
keywords = {()},
mendeley-groups = {Prosjektoppgave},
title = {{Selective Search for Object Recognition}},
year = {2012}
}
@misc{Hog2,
author = {{Van Etten}, Adam},
keywords = {HOG},
mendeley-groups = {Prosjektoppgave},
mendeley-tags = {HOG},
title = {{Object Detection in Satellite Imagery, a Low Overhead Approach, Part II}},
year = {2016}
}
@misc{Hog1,
abstract = {HOG},
author = {{Van Etten}, Adam},
keywords = {HOG},
mendeley-groups = {Prosjektoppgave},
mendeley-tags = {HOG},
title = {{Object Detection in Satellite Imagery, a Low Overhead Approach, Part I}},
year = {2016}
}
@misc{VanEtten,
author = {{Van Etten}, Adam},
mendeley-groups = {Prosjektoppgave},
title = {{Object Detection in Satellite Imagery, a Low Overhead Approach, Part I}},
year = {2016}
}
@misc{YOLT,
author = {{Van Etten}, Adam},
mendeley-groups = {Prosjektoppgave},
title = {{You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks}},
year = {2016}
}
@article{Xiao,
abstract = {Salient object detection is a fundamental problem and has been received a great deal of attentions in computer vision. Recently deep learning model became a powerful tool for image feature extraction. In this paper, we propose a multi-scale deep neural network (MSDNN) for salient object detection. The proposed model first extracts global high-level features and context information over the whole source image with recurrent convolutional neural network (RCNN). Then several stacked deconvolutional layers are adopted to get the multi-scale feature representation and obtain a series of saliency maps. Finally, we investigate a fusion convolution module (FCM) to build a final pixel level saliency map. The proposed model is extensively evaluated on four salient object detection benchmark datasets. Results show that our deep model significantly outperforms other 12 state-of-the-art approaches.},
author = {Xiao, Fen and Deng, Wenzheng and Peng, Liangchan and Cao, Chunhong and Hu, Kai and Gao, Xieping},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao et al. - Unknown - MSDNN Multi-Scale Deep Neural Network for Salient Object Detection.pdf:pdf},
keywords = {fusion convolution module,multi-scale feature representation,recurrent convolution neural network,salient object detection},
mendeley-groups = {Prosjektoppgave},
title = {{MSDNN: Multi-Scale Deep Neural Network for Salient Object Detection}},
}
@article{Zeiler2013,
abstract = {Large Convolutional Network models have recently demonstrated impressive classifica-tion performance on the ImageNet bench-mark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be im-proved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of inter-mediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architec-tures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
author = {Zeiler, Matthew D and Fergus, Rob},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2013 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2013}
}
@article{Zhang2016,
abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated col-orizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The sys-tem is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a " colorization Turing test, " asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32{\%} of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learn-ing benchmarks.},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Isola, Efros - Unknown - Colorful Image Colorization.pdf:pdf},
keywords = {CNNs,Colorization,Self-supervised learning,Vision for Graphics},
mendeley-groups = {Prosjektoppgave},
title = {{Colorful Image Colorization}},
year = {2016}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {Prosjektoppgave},
month = {may},
number = {PART 5},
pages = {740--755},
pmid = {16190471},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Everingham2012,
author = {Everingham, Mark and Winn, John},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Everingham, Winn - 2012 - The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit}},
year = {2012}
}
@article{Everingham2007,
author = {Everingham, Mark and Winn, John},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Everingham, Winn - 2007 - The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Development Kit.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Development Kit}},
year = {2007}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
year = {2014}
}
@article{Deng2009,
abstract = {The explosion of image data on the Internet has the po-tential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with im-ages and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called " ImageNet " , a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the se-mantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Construct-ing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechan-ical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, im-age classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical struc-ture of ImageNet can offer unparalleled opportunities to re-searchers in the computer vision community and beyond.},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - Unknown - ImageNet A Large-Scale Hierarchical Image Database.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
year = {2009}
}
@article{YOLOv3,
abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP 50 in 51 ms on a Titan X, com-pared to 57.5 AP 50 in 198 ms by RetinaNet, similar perfor-mance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
author = {Redmon, Joseph and Farhadi, Ali},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmon, Farhadi - Unknown - YOLOv3 An Incremental Improvement.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{YOLOv3: An Incremental Improvement}},
year = {2018}
}
@article{Viola2004,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distin-guished by three key contributions. The first is the introduction of a new image representation called the Integral Image which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small num-ber of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a cascade which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detec-tion the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, P ; and Jones},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2004 - Rapid Object Detection Using a Boosted Cascade of Simple Features.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Rapid Object Detection Using a Boosted Cascade of Simple Features}},
year = {2004}
}
@misc{autonom_trd,
mendeley-groups = {Prosjektoppgave},
title = {{Verdens f{\o}rste f{\o}rerl{\o}se passasjerferge kan g{\aa} over en kanal i Trondheim - Tu.no}},
urldate = {2018-05-09},
author = {Stensvold, T.},
year = {2016}
}
@misc{yara,
mendeley-groups = {Prosjektoppgave},
title = {{Autonomous ship project, key facts about YARA Birkeland - Kongsberg Maritime}},
url = {https://www.km.kongsberg.com/ks/web/nokbg0240.nsf/AllWeb/4B8113B707A50A4FC125811D00407045?OpenDocument},
urldate = {2018-05-09},
author = {Kongsberg},
year = {2017}
}

@misc{cnn_stanford,
mendeley-groups = {Prosjektoppgave},
title = {{Stanford University CS231n: Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.stanford.edu/},
urldate = {2018-05-09},
author = {StanfordUniversity},
year = {2018}
}

@article{Prasad2016,
abstract = {We present a survey on maritime object detection and tracking approaches, which are essential for the development of a navigational system for autonomous ships. The electro-optical (EO) sensor considered here is a video camera that operates in the visible or the infrared spectra, which conventionally complement radar and sonar and have demonstrated effectiveness for situational awareness at sea has demonstrated its effectiveness over the last few years. This paper provides a comprehensive overview of various approaches of video processing for object detection and tracking in the maritime environment. We follow an approach-based taxonomy wherein the advantages and limitations of each approach are compared. The object detection system consists of the following modules: horizon detection, static background subtraction and foreground segmentation. Each of these has been studied extensively in maritime situations and has been shown to be challenging due to the presence of background motion especially due to waves and wakes. The main processes involved in object tracking include video frame registration, dynamic background subtraction, and the object tracking algorithm itself. The challenges for robust tracking arise due to camera motion, dynamic background and low contrast of tracked object, possibly due to environmental degradation. The survey also discusses multisensor approaches and commercial maritime systems that use EO sensors. The survey also highlights methods from computer vision research which hold promise to perform well in maritime EO data processing. Performance of several maritime and computer vision techniques is evaluated on newly proposed Singapore Maritime Dataset.},
archivePrefix = {arXiv},
arxivId = {1611.05842},
author = {Prasad, D. K. and Rajan, D. and Rachmawati, L. and Rajabaly, E. and Quek, C.},
eprint = {1611.05842},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prasad et al. - 2016 - Video Processing from Electro-optical Sensors for Object Detection and Tracking in Maritime Environment A Survey.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
month = {nov},
title = {{Video Processing from Electro-optical Sensors for Object Detection and Tracking in Maritime Environment: A Survey}},
year = {2016}
}
@article{Prasad2016a,
abstract = {—This paper discusses the technical challenges in mar-itime image processing and machine vision problems for video streams generated by cameras. Even well documented problems of horizon detection and registration of frames in a video are very challenging in maritime scenarios. More advanced problems of background subtraction and object detection in video streams are very challenging. Challenges arising from the dynamic nature of the background, unavailability of static cues, presence of small objects at distant backgrounds, illumination effects, all contribute to the challenges as discussed here.},
author = {Prasad, Dilip K and Prasath, C Krishna and Rajan, Deepu and Rachmawati, Lily and Rajabally, Eshan and Quek, Chai},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prasad et al. - 2016 - Challenges in video based object detection in maritime scenario using computer vision.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
title = {{Challenges in video based object detection in maritime scenario using computer vision}},
year = {2016}
}
@incollection{Bay2006,
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1007/11744023_32},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bay, Tuytelaars, Van Gool - 2006 - SURF Speeded Up Robust Features.pdf:pdf},
mendeley-groups = {Prosjektoppgave},
pages = {404--417},
publisher = {Springer, Berlin, Heidelberg},
title = {{SURF: Speeded Up Robust Features}},
year = {2006}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of affine dis-tortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be cor-rectly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual fea-tures to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a sin-gle object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
journal = {International Journal of Computer Vision},
mendeley-groups = {Prosjektoppgave},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
year = {2004}
}

<bibtex>

@article{Tran2016,
abstract = {{\textcopyright} 2016 IEEE. Automatic boat detection plays an important role for maritime surveillance. However, maritime environment represents lots of challenges such as wave of water, boat movements, and weather condition. This paper presents a method for detecting moving boats from sequence of images. We reply on background subtraction (BS) techniques to detect main movements in the scene. However, as the BS algorithms detect only motion pixels, they cannot detect when boats stay immobile for a moment or move slightly. In addition, the BS results often suffer strongly from water waves as well as boat wake. To overcome these drawbacks we detect salient regions using saliency detection techniques (SD) on images. We then apply dynamic fusion technique on BS and SD results for a final boat detection. Experiments have been conducted on a very challenging dataset and show promising results (detection rate achieved to 89{\%}).},
author = {Tran, Thanh Hai and Le, Thi Lan},
doi = {10.1109/ELINFOCOM.2016.7563033},
file = {:home/simenvg/Downloads/ICEIC2016{\_}CameraReady.pdf:pdf},
isbn = {9781467380164},
journal = {International Conference on Electronics, Information, and Communications, ICEIC 2016},
keywords = {background subtration,boat detection,maritime surveillance,saliency detection},
mendeley-groups = {Master},
number = {January 2016},
title = {{Vision based boat detection for maritime surveillance}},
year = {2016}
}
@article{BackgroundWaveletSubstract,
abstract = {Moving object segmentation in maritime domain is a challenging task due to the various real time practical problems, such as waves on the water surface, boat wakes and weather issues (such as bright sun, fog and heavy rain). These problems contribute to generate a highly dynamic back-ground, gradual and sudden illumination changes, camera jitter, shadows and reflections that can provoke false detections. To deal with above issues, in this paper a fast and robust moving object segmentation method on a water surface for maritime surveillance using dynamic background modeling and shadow suppression (OSDBMSS) in the complex wavelet domain is proposed. For dynamic background modeling, we have used frame difference, background registration, back-ground difference and background difference mask in the complex wavelet domain. For shadow detection and removal, we exploit the high frequency subband in the complex wavelet domain. A comparative analysis of the proposed method is presented both qualitatively and quantitatively with other standard methods available in the literature for publicly available datasets of videos in differ-ent maritime scenarios, with varying light and weather conditions. Experimental results indicate that the proposed method is performing better in comparison to other standard methods for all the test cases.},
author = {Kumar, Alok and Kushwaha, Singh and Srivastava, Rajeev},
doi = {10.1093/comjnl/bxv091},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kumar, Kushwaha, Srivastava - Unknown - Maritime Object Segmentation Using Dynamic Background Modeling and Shadow Suppression.pdf:pdf},
keywords = {change and shadow detection,complex wavelet domain,dynamic background modeling,maritime video surveillance,moving object segmentation},
mendeley-groups = {Prosjektoppgave},
title = {{Maritime Object Segmentation Using Dynamic Background Modeling and Shadow Suppression}},
year = {2015}
}
@article{HOGdetection,
author = {Eum, Hyukmin and Bae, Jaeyun and Yoon, Changyong and Kim, Euntai},
file = {:home/simenvg/Desktop/ijfis-15-251.pdf:pdf},
keywords = {edge-based segmentation,histogram of oriented gradient,ship,ship detection,size ratio,support vector machine},
mendeley-groups = {Master},
number = {4},
pages = {251--259},
title = {{Ship Detection Using Edge-Based Segmentation and Histogram of Oriented Gradient with Ship Size Ratio}},
volume = {15},
year = {2015}
}
@article{HAARdetection,
abstract = {In this paper an automatic maritime surveillance system is presented. Boat detection is performed by means of an Haar-like classifier in order to obtain robustness with respect to targets having very different size, reflections and wakes on the water surface, and apparently motionless boats anchored off the coast. Detection results are filtered over the time in order to reduce the false alarm rate. Experimental results show the effectiveness of the approach with different light conditions and camera positions. The system is able to provide the user a global view adding a visual dimension to AIS data.},
author = {Bloisi, D. and Iocchi, L. and Fiorini, M. and Graziano, G.},
file = {:home/simenvg/Desktop/bloisi-maritime-surveillance.pdf:pdf},
journal = {International Defense and Homeland Security Simulation Workshop, DHSS 2011, Held at the International Mediterranean and Latin American Modeling Multiconference, I3M 2011},
keywords = {Border control,Coastal surveillance,Data fusion,Territorial waters},
mendeley-groups = {Master},
number = {c},
pages = {141--145},
title = {{Automatic maritime surveillance with visual target detection}},
year = {2011}
}

@article{SeeCoast,
abstract = {SeeCoast is a prototype US Coast Guard (USCG) port surveillance system that provides automated scene understanding support for watchstanders. A major SeeCoast objective is to reduce operator workload while maintaining optimal domain awareness by shifting operators' focus from having to detect events to being able to analyze and act upon the knowledge derived from automatically detected anomalous activities. Analyst-defined vessel activities are recognized from pre-scripted patterns and anomalous vessel activities are detected using machine learning techniques. The baseline SeeCoast system interfaces to the USCG Hawkeye prototype and uses (a) machine vision technology to produce target tracks from streaming video data; (b) multi-INT fusion technology to correlate radar, automatic identification system (AIS), and/or video track data into a single coherent track picture; (c) vessel activity analysis and learning technology to provide alerts for events of interest according to user-defined criteria; and (d) visualization of those alerts embedded within the common operating picture. The video processing component tasks and controls Hawkeye cameras to detect vessels in motion and generates vessel track and classification (based on vessel length) information. SeeCoast detects unsafe, illegal, and threatening vessel activities using a rule-based pattern recognizer and detects anomalous vessel activities on the basis of automatically learned behavior normalcy models. Operators can optionally guide the learning system in the form of examples and counter-examples of activities of interest, and refine the performance of the learning system by confirming alerts or indicating examples of false alarms. This paper focuses on the learning-based activity analysis capabilities of SeeCoast},
author = {Rhodes, Bradley J. and Bomberger, Neil A. and Seibert, Michael and Waxman, Allen M.},
doi = {10.1109/MILCOM.2006.302306},
file = {:home/simenvg/Desktop/04086588.pdf:pdf},
isbn = {1424406188},
issn = {2155-7578},
journal = {Proceedings - IEEE Military Communications Conference MILCOM},
keywords = {Anomaly detection,Automated alerting,Homeland security,Normalcy learning,Port security},
mendeley-groups = {Master},
title = {{SeeCoast: Automated port scene understanding facilitated by normalcy learning}},
year = {2007}
}


@article{Pires2010,
author = {Pires, Nuno and Guinet, Jonathan and Dusch, Elodie},
file = {:home/simenvg/Desktop/20100521204750{\_}OCOSS{\_}ASV{\_}Guinet.pdf:pdf},
journal = {Navigation},
mendeley-groups = {Master},
number = {232},
pages = {1--9},
title = {{ASV : An innovative automatic system for maritime surveillance}},
volume = {58},
year = {2010}
}
@article{Wedel2007,
abstract = {Much work was carried out recently for emergency braking based on radar signals. The key step for emergency braking is the reliable detection of obstacles. Moving objects are verified as such by tracking the radar signal. However, discarding so-called phantom objects remains a challenge for stationary objects. This leads to the question of sensor fusion for more reliable verification of obstacles. In this paper we propose a novel method using a monocular camera, such as the night view camera in the Mercedes S class. Our two goals in this paper are the verification of obstacles and the detection of obstacle boundaries. This allows analysis of the situation for carrying out emergency braking. The verification of obstacles is done by analyzing the scaling of obstacles as they get closer to the camera. The perspective image motion of the ground plane serves as a counter hypothesis to detect phantom objects. Obstacle boundaries are found by graph cut segmentation on these two motion fields.},
author = {Wedel, a. and Franke, U.},
doi = {10.1109/IVS.2007.4290097},
file = {:home/simenvg/Desktop/10.1.1.125.5393.pdf:pdf},
isbn = {1-4244-1067-3},
issn = {1931-0587},
journal = {2007 IEEE Intelligent Vehicles Symposium},
mendeley-groups = {Master},
pages = {93--98},
title = {{Monocular Video serves RADAR-based Emergency Braking}},
year = {2007}
}

@inproceedings{RIBDetection,
abstract = {Protection of naval bases and harbors requires close co-operation between security and access control systems covering
land areas and those monitoring sea approach routes. The typical location of naval bases and harbors - usually next to a
large city - makes it difficult to detect and identify a threat in the dense regular traffic of various sea vessels (i.e.
merchant ships, fishing boats, tourist ships). Due to the properties of vessel control systems, such as AIS (Automatic
Identification System), and the effectiveness of radar and optoelectronic systems against different targets it seems that
fast motor boats called RIB (Rigid Inflatable Boat) could be the most serious threat to ships and harbor infrastructure. In
the paper the process and conditions for the detection and identification of high-speed boats in the areas of ports and
naval bases in the near, medium and far infrared is presented. Based on the results of measurements and recorded
thermal images the actual temperature contrast delta T (RIB / sea) will be determined, which will further allow to specify
the theoretical ranges of detection and identification of the RIB-type targets for an operating security system. The data
will also help to determine the possible advantages of image fusion where the component images are taken in different
spectral ranges. This will increase the probability of identifying the object by the multi-sensor security system equipped
additionally with the appropriate algorithms for detecting, tracking and performing the fusion of images from the visible
and infrared cameras.},
author = {Dulski, R. and Milewski, S. and Kastek, M. and Trzaskawka, P. and Szustakowski, M. and Ciurapinski, W. and Zyczkowski, M.},
doi = {10.1117/12.898272},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dulski et al. - 2011 - Detection of small surface vessels in near, medium, and far infrared spectral bands(2).pdf:pdf},
keywords = {DRI ranges,IR detection,multispectral detection,small vessels detection},
mendeley-groups = {master (1),Master},
month = {oct},
pages = {81850U},
publisher = {International Society for Optics and Photonics},
title = {{Detection of small surface vessels in near, medium, and far infrared spectral bands}},
volume = {8185},
year = {2011}
}

@article{KITTI,
abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
archivePrefix = {arXiv},
arxivId = {1612.07695},
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6248074},
eprint = {1612.07695},
file = {:home/simenvg/Desktop/Geiger2012CVPR.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Master},
pages = {3354--3361},
pmid = {15747803},
title = {{Are we ready for autonomous driving? the KITTI vision benchmark suite}},
year = {2012}
}

@article{Sun2006,
abstract = {Developing on-board automotive driver assistance systems aiming to alert drivers about driving environments, and possible collision with other vehicles has attracted a lot of attention lately. In these systems, robust and reliable vehicle detection is a critical step. This paper presents a review of recent vision-based on-road vehicle detection systems. Our focus is on systems where the camera is mounted on the vehicle rather than being fixed such as in traffic/driveway monitoring systems. First, we discuss the problem of on-road vehicle detection using optical sensors followed by a brief review of intelligent vehicle research worldwide. Then, we discuss active and passive sensors to set the stage for vision-based vehicle detection. Methods aiming to quickly hypothesize the location of vehicles in an image as well as to verify the hypothesized locations are reviewed next. Integrating detection with tracking is also reviewed to illustrate the benefits of exploiting temporal continuity for vehicle detection. Finally, we present a critical overview of the methods discussed, we assess their potential for future deployment, and we present directions for future research.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Sun, Zehang and Bebis, George and Miller, Ronald},
doi = {10.1109/TPAMI.2006.104},
eprint = {9605103},
file = {:home/simenvg/Desktop/01608034.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer vision,Intelligent vehicles,Vehicle detection},
mendeley-groups = {Master},
number = {5},
pages = {694--711},
pmid = {16640257},
primaryClass = {cs},
title = {{On-road vehicle detection: A review}},
volume = {28},
year = {2006}
}

@article{SSD_detection2018,
abstract = {This paper aims at developing a real-time vessel detection and tracking system using surveillance cameras in harbours with the purpose to improve the current Vessel Tracking Systems (VTS) performance. To this end, we introduce a novel maritime dataset, containing 70,513 ships in 48,966 images, covering 10 camera viewpoints indicating real-life ship traffic situations. For detection, a Convolutional Neural Network (CNN) detector is trained, based on the Single Shot Detector (SSD) from literature. This detector is modified and enhanced to support the detection of extreme variations of ship sizes and aspect ratios. The modified SSD detector offers a high detection performance, which is based on explicitly exploiting the aspect-ratio characteristics of the dataset. The performance of the original SSD detector trained on generic object detection datasets (including ships) is significantly lower, showing the added value of a novel surveillance dataset for ships. Due to the robust performance of over 90{\%} detection, the system is able to accurately detect all types of vessels. Hence, the system is considered a suitable complement to conventional radar detection, leading to a better operational picture for the harbour authorities.},
author = {Zwemer, Matthijs H and Wijnhoven, Rob G J and {De With}, Peter H N},
doi = {10.5220/0006541501530160},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zwemer, Wijnhoven, De With - 2018 - Ship Detection in Harbour Surveillance based on Large-Scale Data and CNNs.pdf:pdf},
keywords = {CNN,Harbour Surveillance,Object Detection,SSD Detector,Ships,Vessel Tracking System},
mendeley-groups = {master (1),Master},
title = {{Ship Detection in Harbour Surveillance based on Large-Scale Data and CNNs}},
year = {2018}
}
@techreport{Imagenet,
abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called "ImageNet", a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
file = {::},
mendeley-groups = {master (1)},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
year = {2009}
}

@techreport{COCO,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0312v3},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C Lawrence and Dol{\'{i}}, Piotr},
eprint = {arXiv:1405.0312v3},
file = {::},
mendeley-groups = {master (1)},
title = {{Microsoft COCO: Common Objects in Context}},
}
@techreport{MaskRCNN,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing , single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/ facebookresearch/Detectron.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.06870v3},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
eprint = {arXiv:1703.06870v3},
file = {::},
mendeley-groups = {master (1)},
title = {{Mask R-CNN}},
year = {2018}
}
@techreport{Multibox,
abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
file = {::},
mendeley-groups = {master (1)},
title = {{Scalable Object Detection using Deep Neural Networks}},
year = {2014}
}
@techreport{Retinanet,
abstract = {0 0.2 0.4 0.6 0.8 1 probability of ground truth class 0 1 2 3 4 5 loss = 0 = 0.5 = 1 = 2 = 5 well-classiied examples well-classiied examples CE(pt) = − log(pt) FL(pt) = −(1 − pt) $\gamma$ log(pt) Figure 1. We propose a novel loss we term the Focal Loss that adds a factor (1 − pt) $\gamma$ to the standard cross entropy criterion. Setting $\gamma$ {\textgreater} 0 reduces the relative loss for well-classified examples (pt {\textgreater} .5), putting more focus on hard, misclassified examples. As our experiments will demonstrate, the proposed focal loss enables training highly accurate dense object detectors in the presence of vast numbers of easy background examples. Abstract The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron. 50 100 150 200 250 inference time (ms) 28 30 32 34 36 38 COCO AP B C D E F G RetinaNet-50 RetinaNet-101 AP time [A] YOLOv2 † [27] 21.6 25 [B] SSD321 [22] 28.0 61 [C] DSSD321 [9] 28.0 85 [D] R-FCN ‡ [3] 29.9 85 [E] SSD513 [22] 31.2 125 [F] DSSD513 [9] 33.2 156 [G] FPN FRCN [20] 36.2 172 RetinaNet-50-500 32.5 73 RetinaNet-101-500 34.4 90 RetinaNet-101-800 37.8 198 † Not plotted ‡ Extrapolated time Figure 2. Speed (ms) versus accuracy (AP) on COCO test-dev. Enabled by the focal loss, our simple one-stage RetinaNet detector outperforms all previous one-stage and two-stage detectors, including the best reported Faster R-CNN [28] system from [20]. We show variants of RetinaNet with ResNet-50-FPN (blue circles) and ResNet-101-FPN (orange diamonds) at five scales (400-800 pixels). Ignoring the low-accuracy regime (AP{\textless}25), RetinaNet forms an upper envelope of all current detectors, and an improved variant (not shown) achieves 40.8 AP. Details are given in {\S}5.},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.02002v2},
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'{a}}r, Piotr},
eprint = {arXiv:1708.02002v2},
file = {::},
mendeley-groups = {master (1)},
title = {{Focal Loss for Dense Object Detection}},
year = {2018}
}

@article{Dettmers2015,
author = {Dettmers, Tim},
title = {{Deep Learning in a Nutshell: History and Training}},
year = {2015}
}
@article{TransferLearning,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multi-task learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
author = {Pan, Sinno Jialin and Yang, Qiang},
doi = {10.1109/TKDE.2009.191},
file = {::},
keywords = {Data Mining,Index Terms-Transfer Learning,Machine Learning,Survey},
title = {{A Survey on Transfer Learning}},
year = {2009}
}

@techreport{TransferLearning2,
abstract = {As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
archivePrefix = {arXiv},
arxivId = {1808.01974v1},
author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
eprint = {1808.01974v1},
file = {::},
isbn = {1808.01974v1},
keywords = {Deep Transfer Learning,Survey,Transfer Learning},
title = {{A Survey on Deep Transfer Learning}},
year = {2018}
}

@article{Kamsvag2018,
author = {Kamsv{\aa}g, Vegard},
keywords = {Kybernetikk og robotikk (2 {\aa}rig),Master thesis},
publisher = {NTNU},
title = {{Fusion between camera and lidar for autonomous surface vehicles}},
year = {2018}
}

@article{Bohn2018,
author = {B{\o}hn, Eivind},
keywords = {Kybernetikk og robotikk (2 {\aa}rig),Master thesis},
publisher = {NTNU},
title = {{Semantic Segmentation of Radar Data with Deep Learning}},
year = {2018}
}

@article{Markov,
abstract = {A new Bayesian state and parameter learning algorithm for multiple target tracking (MTT) models with image observations is proposed. Specifically, a Markov chain Monte Carlo algorithm is designed to sample from the posterior distribution of the unknown number of targets, their birth and death times, states and model parameters, which constitutes the complete solution to the tracking problem. The conventional approach is to pre-process the images to extract point observations and then perform tracking. We model the image generation process directly to avoid potential loss of information when extracting point observations. Numerical examples show that our algorithm has improved tracking performance over commonly used techniques, for both synthetic examples and real florescent microscopy data, especially in the case of dim targets with overlapping illuminated regions.},
archivePrefix = {arXiv},
arxivId = {1603.05522},
author = {Jiang, Lan and Singh, Sumeetpal S.},
eprint = {1603.05522},
file = {::},
month = {mar},
title = {{Tracking multiple moving objects in images using Markov Chain Monte Carlo}},
year = {2016}
}

@techreport{VanEtten2018,
abstract = {Detection of small objects in large swaths of imagery is one of the primary problems in satellite imagery analytics. While object detection in ground-based imagery has beneeted from research into new deep learning approaches, transitioning such technology to overhead imagery is nontrivial. Among the challenges is the sheer number of pixels and geographic extent per image: a single DigitalGlobe satellite image encompasses {\textgreater} 64 km 2 and over 250 million pixels. Another challenge is that objects of interest are minuscule (ooen only ∼ 10 pixels in extent), which complicates traditional computer vision techniques. To address these issues, we propose a pipeline (You Only Look Twice, or YOLT) that evaluates satellite images of arbitrary size at a rate of ≥ 0.5 km 2 /s. .e proposed approach can rapidly detect objects of vastly diierent scales with relatively liile training data over multiple sensors. We evaluate large test images at native resolution, and yield scores of F 1 {\textgreater} 0.8 for vehicle localization. We further explore resolution and object size requirements by systematically testing the pipeline at decreasing resolution, and conclude that objects only ∼ 5 pixels in size can still be localized with high conndence. Code is available at hhps://github.com/CosmiQ/yolt},
archivePrefix = {arXiv},
arxivId = {1805.09512v1},
author = {{Van Etten}, Adam},
eprint = {1805.09512v1},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Eeen - Unknown - You Only Look Twice Rapid Multi-Scale Object Detection In Satellite Imagery.pdf:pdf},
keywords = {Computer Vision,Object Detection,Satellite Imagery},
mendeley-groups = {master (1)},
title = {{You Only Look Twice: Rapid Multi-Scale Object Detection In Satellite Imagery}},
year = {2018}
}

@article{Goring2017,
author = {Goring, Robert},
journal = {Dissertations and Theses},
mendeley-groups = {master (1)},
month = {apr},
title = {{Feasibility of Neural Networks for Maritime Visual Detection on a Mobile Platform}},
year = {2017}
}


@techreport{Challenges2016,
abstract = {This paper discusses the technical challenges in maritime image processing and machine vision problems for video streams generated by cameras. Even well documented problems of horizon detection and registration of frames in a video are very challenging in maritime scenarios. More advanced problems of background subtraction and object detection in video streams are very challenging. Challenges arising from the dynamic nature of the background, unavailability of static cues, presence of small objects at distant backgrounds, illumination effects, all contribute to the challenges as discussed here.},
archivePrefix = {arXiv},
arxivId = {1608.01079v1},
author = {Prasad, Dilip K and Prasath, C Krishna and Rajan, Deepu and Rachmawati, Lily and Rajabally, Eshan and Quek, Chai},
eprint = {1608.01079v1},
file = {:home/simenvg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Prasad et al. - Unknown - Challenges in video based object detection in maritime scenario using computer vision.pdf:pdf},
isbn = {1608.01079v1},
mendeley-groups = {master (1)},
title = {{Challenges in video based object detection in maritime scenario using computer vision}},
year = {2016}
}







</bibtex>
